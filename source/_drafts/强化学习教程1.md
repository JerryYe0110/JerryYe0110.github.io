---
title: 强化学习教程1
date: 2021-06-21 15:52:47
tags:
  - rl
---
深度强化学习第一讲
<!--More-->

# Terminology

1. state **s**  (this frmae)
2. **agent**, do **action**
3. policy $\pi$, $\pi (s, a) --> [0, 1]: \pi (a|s) = P(A=a|S=s)$, 为概率密度函数， 随机的意义是为了对抗的时候赢
4. reward ， 强化学习的目标是为了获得的奖励更高
5. state transition `old state --action--> new state` 随机的操作. $\pi (s'|s, a) = P(S'=s'|S=s, A=a)$， 我们不知道，这是环境的操作
6. trajectory: (state, action, reward), 组成一个轨迹，多个轨迹
7. Return(aka cumulative future reward), $U_t = R_t + R_{t+1}....$
8. discount Return(aka cumulative future reward), $U_t = R_t + a*R_{t+1} +a^2 R_{t+2}....$
9. Action-value function: $Q(s_t, a_t) = E[U_t|S_t = s_t, A_t = a_t]$, 使discount return 未来项都被积分掉，只留下当前状态的选择，因为未来的选择是根据概率函数随机选择的，也以一定的比例在可以积, 只和$s_t, a_t， \pi$有关
10. Optimal action-value function, 这个与$\pi$无关
11. state-value function $V_{\pi}(s_t) = E_A[Q_{\pi}(s_t, A)$, 只和 $\pi$ 和 s 有关，表示现在局面是否好, 把当前状态的a也积分掉


# AI如何控制agent

1. 策略学习， $\pi(a|s)$, 我们学习一个好的p，然后我们输入一个状态s进去，然后根据p随机采样一个a
2. 价值学习， $Q^*(s, a)$, 我们输入一个状态s， 然后我们有一个价值函数，判断哪个a更好，选择价值最大的动作

3. 经典控制问题
4. Atari games
5. MuJoCo 


# 使用gym的代码

```python
import gym
env = gym.make('CartPole-v0') # exv provides s and r

state = env.reset()
for t in range(100):
  env.render() #渲染
  print(state)
  
  action = env.action_space.sample() # 实际上是根据 policy 获得的action， 这里随机拿点
  state, reward, done, info = env.step(action) # 进入环境
  
  if done: #游戏wn or lose 就会为 1
	 print('Finished')
	 break
	 
env.close()
```

