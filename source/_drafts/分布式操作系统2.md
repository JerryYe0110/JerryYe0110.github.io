---
title: 分布式机器学习2
date: 2021-06-19 19:06:14
tags:
  - ml
---
该章节主要介绍机器学习的常用损失函数，基本流程和一些机器学习理论的简单介绍
<!--More-->

# 机器学习的基本流程

对于一个数据集（有监督学习），我们把数据分成两部分，一部分是样们特征，每个样本可以被表示为$d$维的向量，这个样本被赋予一个特征$y_i$，对于评判一个分类模型是否好，最终的评价标准就是准确率，我们认为错误率地的模型的经验误差风险很小，但是这个是一个非连续的评价标准，不易优化，所以引出了损失函数。机器学习算法主要实现对数据的特征提取，使用损失函数优化参数，使用模型预测标签。

# 损失函数

## Hinge 损失函数

在二分类问题上，我们将标签设置为 $[-1, 1]$ 上使用Hinge损失 $max(0, 1- \hat{y} *y)$ 会发现，如果预测结果相反，那么他的损失值会很大，当预测结果相同，且预测结果和真实结果$\hat{y}*y>1$时，误差就会被认为为0

## 指数损失函数

在二分类问题上，我们将标签设置为 $[-1, 1]$ 上使用指数损失 $exp(-y*\hat{y})$ 会发现，如果预测结果相反，那么他的损失值会很大，比*Hinge* 的惩罚力度还要大，u因为他是指数阶的惩罚，那个是线性阶的惩罚

## 交叉熵损失函数

比较熟悉，不多赘述

# 常用的机器学习模型

## 线性模型

线性模型可以通常的表示为$w^Tx$, 偏置项$b$可以通过在$x$上多加一维的常数维来表示，线性模型往往有很好的解释性，因为每一个维度都表示一个维度的重要性，但是线性模型也有很大的局限性。

其一是其上下界无穷，他可以很大，但是很多问题是$[0, 1]$区间内问题。 其二， 线性模型的表达力有限。 为了解决问题一，我们会引入激活函数如，对数几率函数 $g(x ; w) = \frac{1}{1+exp(-w^Tx)}$ 这个变换过的函数取值在$[0, 1]$，这类函数我们称为广义线性函数。对于问题二我们可以对输入的特征本身做一个非线性预变换（如交叉乘积，对数，多项式变换等）， 或者使用核方法（支持向量机）。

我们这里介绍下核方法， **基本思想是通过非线性变换，将数据映射到更高维的空间（希尔伯特空间），解决原始问题的线性不可分问题** 

## 决策树与Boosting

决策数通常会包括根节点，若干内部节点和一些叶子节点。 叶子节点就是决策节点。 因为在每个节点的操作都是非线性的，所以决策树可以实现较复杂的非线性映射。 常见的决策树算法的基本流程都比较类似，有选择和减枝的过程。

选择可以理解为前向传播，根据某个准则，将某个数据集内的样本分到他的子树上，常用的准则有： 信息增益，增益率和基尼系数等，思想其实都是信息是否变多， 信息越多，概率越固定。最后的目标是找到使划分后平均信息熵最小的特征j， 让信息增益最大。

剪枝是为了抑制过拟合（有点像dropout的思想）， 剪枝包括预剪枝和后剪枝，预是在训练时就估计（用交叉验证集），如果不能提高泛化能力的话，就不进行继续的划分， 后是在全部完成后，自底向上考虑如果去掉某个节点（合并子节点和父节点）是否能提高泛化能力，若能，就合并。

但是如果学习难度比较大，一棵树可能能力不够，会训练很多树，对这些树进行bagging或者boosting。

boosting 的基本思路是先训练出一个弱学习器$h_i(x)$然后根据这个表现去调整训练样本的分布，使错误样本在后续的训练中得到更多的关注，来训练下一个弱学习器， 最后以所有弱学习器的加权组合为最后的预测结果。**这个方法对抗过拟合很好，即使训练集误差已经为0,继续训练仍旧能提高测试集的表现，因为他会将样本间隔变大** 

## 神经网络

使用激活函数，代替阶跃函数（神经元的阈值）。


# 机器学习理论 

一般的，所有机器学习方法都是想要最小化期望损失风险（任意样本上），但真实分布是不可能知道的，所以我们将这个问题转化为在训练集上最小化风险（经验风险）， 相当与我们只允许在一个函数空间受到限制的部分空间搜索局部最优值。

我们对泛化误差进行分解来详细讨论误差。我们希望优化算法的模型$\hat{g_t}$和最优的模型$g^*$对应的期望的差尽可能的小$L(\hat{g_r})-L(g^*)$，

$L(\hat{g_r})-L(g^*) = L(\hat{g_n})-L(\hat{g_n})+L(\hat{g_n}) - L(g_G^*)+L(g_G^*)-L(g^*)$

其中$g^*_G$表示在$G$这个函数这个函数空间里面期望风险最小的模型， $\hat{g_n}$表示经验风险最小的模型，这个是在受限空间内的。

上面那个等式右侧一共三个差，
+ 第一个差表示优化误差，是优化算法能否达到这个数据能达到的上限
+ 第二个是估计误差，这是数据集本身带来的误差，由数据集合本身的局限性
+ 第三个是近似误差，这个误差和函数空间的表达力有关（简单理解，特征的多少）


后面基于容度估计上界完全没看懂，等看了相关论文以后来填吧。

